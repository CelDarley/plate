#!/usr/bin/env python3
"""
Scraper para placas usando Requests + BeautifulSoup (sem navegador)
Muito mais rÃ¡pido e eficiente que Selenium
"""

import requests
from bs4 import BeautifulSoup
import time
import random
from typing import Dict, Optional, List
import re

class PlacaFipeScraperRequests:
    """Scraper de placas usando requisiÃ§Ãµes HTTP diretas"""
    
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://placafipe.com"
        self.search_url = "https://placafipe.com/consulta"
        
        # Headers mais realistas para simular um navegador real
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Linux"'
        }
        
        # Configurar session
        self.session.headers.update(self.headers)
        
        # Adicionar referer e outras configuraÃ§Ãµes
        self.session.verify = True
        self.session.allow_redirects = True
        
    def _get_random_delay(self) -> float:
        """Retorna um delay aleatÃ³rio entre 1-3 segundos"""
        return random.uniform(1, 3)
    
    def _extract_data_from_html(self, html_content: str) -> Dict[str, str]:
        """Extrai dados do HTML usando BeautifulSoup"""
        soup = BeautifulSoup(html_content, 'html.parser')
        dados = {}
        
        try:
            # Procurar pela tabela de resultados
            tabela = soup.find('table', class_='table') or soup.find('table')
            
            if not tabela:
                print("   âš ï¸  Tabela de resultados nÃ£o encontrada")
                return {}
            
            # Extrair linhas da tabela
            linhas = tabela.find_all('tr')
            print(f"   ðŸ“Š Encontradas {len(linhas)} linhas na tabela")
            
            for i, linha in enumerate(linhas):
                # Procurar por cÃ©lulas com label e valor
                celulas = linha.find_all(['td', 'th'])
                if len(celulas) >= 2:
                    # Tentar extrair label e valor
                    label = celulas[0].get_text(strip=True)
                    valor = celulas[1].get_text(strip=True)
                    
                    if label and valor:
                        # Mapear labels para campos do banco
                        campo = self._mapear_campo(label)
                        if campo:
                            dados[campo] = valor
                            print(f"   ðŸ“ {campo}: {valor}")
            
            print(f"   âœ… Dados extraÃ­dos: {len(dados)} campos")
            return dados
            
        except Exception as e:
            print(f"   âŒ Erro ao extrair dados: {str(e)}")
            return {}
    
    def _mapear_campo(self, label: str) -> Optional[str]:
        """Mapeia labels da tabela para campos do banco"""
        label_lower = label.lower()
        
        mapeamento = {
            'marca': 'marca',
            'marcaplacas mercosul': 'marca',
            'genÃ©rico': 'generico',
            'modelo': 'modelo',
            'importado': 'importado',
            'ano': 'ano',
            'ano modelo': 'ano_modelo',
            'cor': 'cor',
            'cilindrada': 'cilindrada',
            'combustÃ­vel': 'combustivel',
            'chassi': 'chassi',
            'motor': 'motor',
            'passageiros': 'passageiros',
            'uf': 'uf',
            'municÃ­pio': 'municipio'
        }
        
        for key, value in mapeamento.items():
            if key in label_lower:
                return value
        
        return None
    
    def scraping_placa(self, placa: str) -> Optional[Dict[str, str]]:
        """Faz scraping de uma placa especÃ­fica"""
        print(f"   ðŸŒ Iniciando scraping para placa: {placa}")
        
        try:
            # Primeira requisiÃ§Ã£o para obter cookies e simular navegaÃ§Ã£o
            print("   ðŸ” Obtendo pÃ¡gina inicial...")
            response = self.session.get(self.base_url, timeout=30)
            print(f"   ðŸ“Š Status da pÃ¡gina inicial: {response.status_code}")
            
            if response.status_code != 200:
                print(f"   âš ï¸  PÃ¡gina inicial retornou status {response.status_code}")
                print(f"   ðŸ“„ ConteÃºdo: {response.text[:200]}...")
                return None
            
            # Aguardar um pouco para simular comportamento humano
            time.sleep(self._get_random_delay())
            
            # Tentar diferentes URLs de consulta
            urls_consulta = [
                f"{self.base_url}/consulta",
                f"{self.base_url}/buscar",
                f"{self.base_url}/",
                f"{self.base_url}/index.php"
            ]
            
            dados = None
            for url in urls_consulta:
                try:
                    print(f"   ðŸ” Tentando URL: {url}")
                    
                    # Preparar dados para a consulta
                    data = {
                        'placa': placa,
                        'submit': 'Consultar',
                        'buscar': 'Consultar',
                        'search': placa
                    }
                    
                    # Fazer a consulta
                    print(f"   ðŸ” Consultando placa: {placa}")
                    response = self.session.post(
                        url,
                        data=data,
                        timeout=30,
                        allow_redirects=True
                    )
                    
                    print(f"   ðŸ“Š Status da consulta: {response.status_code}")
                    print(f"   ðŸ“„ Tamanho da resposta: {len(response.text)} bytes")
                    
                    if response.status_code == 200:
                        # Verificar se a consulta foi bem-sucedida
                        if 'erro' in response.text.lower() or 'nÃ£o encontrada' in response.text.lower():
                            print(f"   âš ï¸  Placa {placa} nÃ£o encontrada ou erro na consulta")
                            continue
                        
                        # Extrair dados do HTML retornado
                        dados = self._extract_data_from_html(response.text)
                        
                        if dados:
                            print(f"   âœ… Dados extraÃ­dos com sucesso para {placa}")
                            break
                        else:
                            print(f"   âš ï¸  Nenhum dado extraÃ­do da URL {url}")
                    else:
                        print(f"   âš ï¸  URL {url} retornou status {response.status_code}")
                        
                except Exception as e:
                    print(f"   âš ï¸  Erro ao tentar URL {url}: {str(e)}")
                    continue
            
            return dados
                
        except requests.exceptions.RequestException as e:
            print(f"   âŒ Erro de requisiÃ§Ã£o para {placa}: {str(e)}")
            return None
        except Exception as e:
            print(f"   âŒ Erro geral para {placa}: {str(e)}")
            return None
    
    def scraping_multiplas_placas(self, placas: List[str], delay_entre_placas: float = 2.0) -> List[Dict[str, str]]:
        """Faz scraping de mÃºltiplas placas com delay entre elas"""
        resultados = []
        
        for i, placa in enumerate(placas):
            print(f"\nðŸš— Processando placa {i+1}/{len(placas)}: {placa}")
            
            dados = self.scraping_placa(placa)
            if dados:
                dados['placa'] = placa
                resultados.append(dados)
            
            # Delay entre placas (exceto na Ãºltima)
            if i < len(placas) - 1:
                print(f"   â° Aguardando {delay_entre_placas} segundos...")
                time.sleep(delay_entre_placas)
        
        return resultados
    
    def close(self):
        """Fecha a sessÃ£o"""
        self.session.close()

# FunÃ§Ã£o de teste
def testar_scraper():
    """FunÃ§Ã£o para testar o scraper"""
    scraper = PlacaFipeScraperRequests()
    
    try:
        # Testar com uma placa
        placa_teste = "ABC1234"
        print(f"ðŸ§ª Testando scraper com placa: {placa_teste}")
        
        dados = scraper.scraping_placa(placa_teste)
        
        if dados:
            print(f"âœ… Sucesso! Dados obtidos: {dados}")
        else:
            print("âŒ Falha ao obter dados")
            
    except Exception as e:
        print(f"âŒ Erro no teste: {str(e)}")
    finally:
        scraper.close()

if __name__ == "__main__":
    testar_scraper()
